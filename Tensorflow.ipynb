{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+mRehUlgezRZ5GIUIk7iX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrajwalSathyanarayana/TensorFlow/blob/main/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCXpYmgoFR4C",
        "outputId": "f70bc0ca-bef4-46d5-fe28-f0436dccc890"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch #import PyTorch library\n",
        "torch.Tensor #refers to Tensor class within the torch library.\n",
        "# A Tensor is a fundamental data structure in PyTorch; it's a multi dimensional array,\n",
        "# similar to NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new tensor in PyTorch.\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "notes = '''\n",
        "[1.0, 2.0] - data that tensor x will hold.\n",
        "requires_grad = True - important argument that enables automatic differentiation.\n",
        "It allows PyTorch to track all operations performed on a tensor, and automatically computes the\n",
        "gradient of any function with respect to this tensor later on.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKQkhRwUF3fs",
        "outputId": "d634c0b2-8420-4e07-fdf3-5658bdc8a5af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x * 2\n",
        "z = y.sum()\n",
        "print(y)\n",
        "print(z)\n",
        "notes = '''\n",
        "- y = x*2 - performs element wise multiplication of tensor x by scalar value 2.\n",
        "- [1.0, 2.0] * 2 = [2.0, 4.0]\n",
        "- Python automatically tracks this multiplication operation;\n",
        "indicating that this tensor y was created as a result of a multiplication operation and PyTorch\n",
        "can compute gradients with respect to it.\n",
        "- z = y.sum() - computes the sum of all elements in the tensor y.\n",
        "- 2.0 + 4.0 = 6.0 i.e., single element tensor.\n",
        "- This sum operation is also tracked.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFbnJQilF7bX",
        "outputId": "94d097ee-b112-4873-e99c-add7faeccd6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 4.], grad_fn=<MulBackward0>)\n",
            "tensor(6., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(z.backward())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEgGfsIrI2D2",
        "outputId": "1d164d73-17e0-4f95-d081-00104cdf569b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jfZ0fNupI7sO",
        "outputId": "6a4d5cce-f2ce-4648-c39b-8dff39ac5ade"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.9.0+cu128'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "CkbnDF73_o9R",
        "outputId": "87a647bc-6abb-4e5f-f81f-72d49822f932"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.is_available() -> bool>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.cuda.is_available</b><br/>def is_available() -&gt; bool</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py</a>Return a bool indicating if CUDA is currently available.\n",
              "\n",
              ".. note:: This function will NOT poison fork if the environment variable\n",
              "    ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` is set. For more details, see\n",
              "    :ref:`multiprocessing-poison-fork-note`.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 163);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor0d = torch.tensor(1) # scalar; 0D tensor\n",
        "tensor1d = torch.tensor([1,2,3]) # vector; 1D tensor\n",
        "tensor2d = torch.tensor([[1,2],\n",
        "                         [3,4]]) # matrix; 2D tensor - rows and columns\n",
        "tensor3d = torch.tensor([[[1,2], [3,4]],\n",
        "                         [[5,6], [7,8]]]) # tensor; 3D tensor - collection of 2D matrics\n",
        "print(tensor0d)\n",
        "print(tensor1d)\n",
        "print(tensor2d)\n",
        "print(tensor3d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtSbvUf0BNbi",
        "outputId": "0ae40a80-639b-483a-9d2a-5ef576297bb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n",
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor0d.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnD0vHfiC_dr",
        "outputId": "057e0be9-a116-40e1-a549-56f130a4a641"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGRESSION FORWARD PASS\n",
        "import torch.nn.functional as F #import the functional module containing various activation, loss functions and other operations\n",
        "y = torch.tensor([1.0]) # true label i.e., actual output\n",
        "x1 = torch.tensor([1.1]) # input feature\n",
        "w1 = torch.tensor([2.2], requires_grad = True) # weight parameter;\n",
        "# requires_grad = true tracks all operations performed on 'w1' tensor.\n",
        "b = torch.tensor([0.0],  requires_grad = True) # bias unit\n",
        "# requires_grad = true tracks all operations performed on 'b' tensor.\n",
        "z = x1 * w1 + b # net input i.e., 1.1 * 2.2 + 0.0 = 2.42\n",
        "a = torch.sigmoid(z) # applies sigmoid activation function to net input 'z'; sigmoid squashes any real valued number into a\n",
        "# range between 0 and 1, making it suitable for binary classification probabilites. 'a' represents the predicted output or\n",
        "# activation of logistic regression model.\n",
        "loss = F.binary_cross_entropy(a,y) # calculate the binary cross entropy loss between predicted output 'a' and true label 'y'\n",
        "print(z)\n",
        "print(a)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV2syGyCDDbu",
        "outputId": "0c93a70e-d3c0-41eb-9c41-e7c176e73c9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.4200], grad_fn=<AddBackward0>)\n",
            "tensor([0.9183], grad_fn=<SigmoidBackward0>)\n",
            "tensor(0.0852, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import grad # import grad function from autograd, allowing the computation\n",
        "# of gradient of a scalar wrt some tensors.\n",
        "grad_L_w1 = grad(loss, w1, retain_graph = True) # calculates the gradient of\n",
        "# loss (scalar o/p from logistic regression forward pass) wrt 'w1' tensor.\n",
        "# retain_graph = True preserves the computational graph for future gradient computation.\n",
        "# In simple terms, it is the rate of change of w1 wrt loss.\n",
        "grad_L_b = grad(loss, b, retain_graph = True) # calculate the gradient of loss wrt 'b' (bias)\n",
        "# tensor. retain_graph = True preserves the computational graph for future gradient computation.\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCfCHWZN0FYR",
        "outputId": "cfb8b62d-2d15-42a0-f85d-5d6b78a36398"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward() # .backward() performs back propogation starting from the specified tensor, here,\n",
        "# loss. It traverses the computational graph & automatically calculates the gradients of loss\n",
        "# with respect to all tensors. Here, it computes d(loss)/d(w1) and d(loss)/d(b)\n",
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G5I9aMc4Awd",
        "outputId": "bcc6a035-ed5a-4284-98e7-f6d43abb000d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTILAYER PERCEPTRON\n",
        "class NeuralNetwork(torch.nn.Module): # custom neural network class that inherits from PyTorch's base Module Class\n",
        "  def __init__(self, num_inputs, num_outputs): # constructor method where the neural network architeecture is defined\n",
        "    super().__init__() # calls the constructor of the parent class (torch.nn.Module)\n",
        "\n",
        "    self.layers = torch.nn.Sequential( # define a Sequence of layers and operations\n",
        "        # HIDDEN LAYER 1\n",
        "        torch.nn.Linear(num_inputs, 30),\n",
        "# first fully connected dense layer.\n",
        "# Takes the input of num_inputs features and transforms them into 30 output features.\n",
        "        torch.nn.ReLU(),\n",
        "# ReLu activation function that intrduces non linearity into the model.\n",
        "# Without a non linear activation function, neural networks will only learn linear relationships.\n",
        "# ReLu outputs the input if its positive, else zero i.e., f(x) = max (0, x)\n",
        "\n",
        "        # HIDDEN LAYER 2\n",
        "        torch.nn.Linear(30, 20),\n",
        "# 30 input features are transformed into 20 output features in the second hidden layer.\n",
        "        torch.nn.ReLU(),\n",
        "\n",
        "        # OUTPUT LAYER\n",
        "        torch.nn.Linear(20, num_outputs),\n",
        "# output layer; 20 features from the previous (last) hidden layer and transforms them into num_outputs features.\n",
        "# num_outputs would typically correspond to the number of classes in a classification problem.\n",
        "    )\n",
        "  def forward(self, x):\n",
        "  # defines the forward pass of the network. Describes how input data 'x' flows\n",
        "  # through the layers to produce an output.\n",
        "    logits = self.layers(x)\n",
        "  # When NeuralNetwork object (ex: model(input_data)) is called, this forward method\n",
        "  # is implicitly executed. The input tensor 'x' is passed sequentially through all the layers defined in self.layers()\n",
        "    return logits"
      ],
      "metadata": {
        "id": "hd6aOKdt7ayf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(50,3)"
      ],
      "metadata": {
        "id": "zdW1V3qR8Ttj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmtVEPUW8dqb",
        "outputId": "c99676cd-3a47-48d8-bcd1-370a9d74c098"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# calculates the total number of trainable parameters within the model\n",
        "# p.numel() - method returns the total number of elements (individual scalar values)\n",
        "# in that parameter tensor. For example, if a weight matrix is 50x30, numel() would return 1500.\n",
        "print(num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avgfAS1s8esp",
        "outputId": "a93ae9ec-637c-4d53-cadf-a617840f2688"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# manual seed is crucial for reproducibility; When you initialize neural network layers\n",
        "# (like torch.nn.Linear), their weights and biases are typically initialized randomly.\n",
        "# By setting a seed, you ensure that every time you run this code, the random\n",
        "# initialization will be the same, leading to identical starting weights and biases.\n",
        "# This makes your experiments consistent and allows others to reproduce your results.\n",
        "model = NeuralNetwork(50,3)\n",
        "print(model.layers[0].weight)\n",
        "print(model.layers[0].bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6irOt19FISE",
        "outputId": "137f2272-31d8-421a-9678-262fd3e32b35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1081,  0.1174, -0.0331,  ...,  0.0253,  0.0718, -0.0862],\n",
            "        [-0.1400, -0.0546, -0.1085,  ..., -0.0477, -0.0501, -0.1368],\n",
            "        [-0.0810,  0.0353, -0.0187,  ...,  0.1142,  0.1288, -0.1121],\n",
            "        ...,\n",
            "        [-0.0031, -0.0573,  0.0515,  ...,  0.0271, -0.0928, -0.1175],\n",
            "        [-0.0444, -0.1318, -0.0660,  ...,  0.0647, -0.1230, -0.0531],\n",
            "        [ 0.0023, -0.1223,  0.0797,  ...,  0.0369,  0.0862,  0.1328]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.1205, -0.0081,  0.0274, -0.0846,  0.0547, -0.1356,  0.0667,  0.0259,\n",
            "         0.0435,  0.0518, -0.0649,  0.1214,  0.0675,  0.0929,  0.0187,  0.0888,\n",
            "         0.0952,  0.0641,  0.1077, -0.0257,  0.0930,  0.0326,  0.0851,  0.1377,\n",
            "         0.0320, -0.0525, -0.0141,  0.0398,  0.0238, -0.0386],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How torch.manual_seed() Ensures Sameness:\n",
        "- Pseudo-random Numbers:\n",
        "  - Computers don't generate truly random numbers. Instead, they use algorithms to generate sequences of numbers that appear random.\n",
        "  - These are called pseudo-random numbers.\n",
        "- The Seed:\n",
        "  - A pseudo-random number generator starts with an initial value called a 'seed'.\n",
        "  - If you provide the same seed to the generator, it will produce the exact same sequence of 'random' numbers every single time.\n",
        "- Reproducible Initialization:\n",
        "  - When you call torch.manual_seed(42), you're telling PyTorch's random number generator to start its sequence with '42'.\n",
        "  - Then, when you create a new NeuralNetwork instance (e.g., model = NeuralNetwork(50,3)), the weights and biases inside its torch.nn.Linear layers are initialized using the next numbers in that fixed pseudo-random sequence.\n",
        "  - Because the sequence is always the same for a given seed, the initial weights and biases of your model will be identical every time you run that code, as long as the seed is set before the model is instantiated."
      ],
      "metadata": {
        "id": "fuz7uZuqLLjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code block demonstrates how to generate a random input, pass it through your\n",
        "# previously defined NeuralNetwork model, and print the resulting output.\n",
        "\n",
        "torch.manual_seed(123)\n",
        "# crucial for ensuring that the random input tensor X generated in the next step is reproducible.\n",
        "\n",
        "X = torch.rand((1,50))\n",
        "# torch.rand() function generates a tensor with random numbers uniformly sampled from the interval [0, 1).\n",
        "#  The (1, 50) argument specifies the shape of the tensor:\n",
        "# it creates a tensor with 1 row and 50 columns. This shape is important because your\n",
        "# NeuralNetwork was initialized with num_inputs=50, meaning it expects an input\n",
        "# tensor with 50 features per sample. The '1' in (1,50) typically represents a batch size\n",
        "# of 1, meaning we are processing a single input sample.\n",
        "\n",
        "out = model(X)\n",
        "# This line performs the forward pass through your NeuralNetwork model.\n",
        "# When you call the model instance like a function (e.g., model(X)),\n",
        "# PyTorch automatically executes the forward method that you defined within your NeuralNetwork class.\n",
        "# The input tensor X goes through all the layers (Linear, ReLU, Linear, ReLU, Linear) in the\n",
        "# self.layers sequential container, and the final output is stored in the out variable.\n",
        "# The out tensor contains the 'logits' from the network's final layer.\n",
        "\n",
        "\n",
        "print(out)\n",
        "# the output will be a tensor of shape (1, 3)\n",
        "# (one sample, three outputs), as your NeuralNetwork was initialized with num_outputs=3."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwcQOcN-I3L4",
        "outputId": "8e944917-ec9b-4e01-9ea0-d7b077ea3ceb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1019, -0.0396, -0.1432]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a small toy dataset\n",
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "\n",
        "y_train = torch.tensor([0,0,0,1,1])\n",
        "\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6]\n",
        "])\n",
        "\n",
        "y_test = torch.tensor([0,1])"
      ],
      "metadata": {
        "id": "0AI6SSnyQJG6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a custom Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "  def __init__(self, X, y): # setup attributes awe can access later in __getitem__ and __len__ methods\n",
        "    self.features = X\n",
        "    self.labels = y\n",
        "\n",
        "  def __getitem__(self, index): # define instructions for returning exactly one item from the dataset via an index.\n",
        "  #this refers to the features and the class label corresponding to a single training example or test instance.\n",
        "    one_x = self.features[index]\n",
        "    one_y = self.labels[index]\n",
        "    return one_x, one_y\n",
        "\n",
        "  def __len__(self): # contains instructions for retrieving the length of the dataset.\n",
        "    return self.labels.shape[0] # .shape used to return the number of rows in the deature array."
      ],
      "metadata": {
        "id": "0mC4XLMmKGdH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "4q-TGc4zKlit"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code block uses the built-in len() function to determine the number of samples in your train_ds and test_ds datasets.\n",
        "print(len(train_ds))\n",
        "print(len(test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feb691pFLgoh",
        "outputId": "f4ca8922-1f96-4383-a979-122d8461e239"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instatntiating data loaders\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_ds,\n",
        "    batch_size = 2, # data should be grouped into batches of 2 samples\n",
        "    shuffle = True, # reshuffle the order of the samples in train_ds at the beginning of each epoch\n",
        "    num_workers = 0 # how many subprocesses to use for data loading. i.e., background processes\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_ds,\n",
        "    batch_size = 2,\n",
        "    shuffle = False,\n",
        "    num_workers = 0\n",
        ")"
      ],
      "metadata": {
        "id": "SHnB_yyALsBT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (x,y) in enumerate(train_loader):\n",
        "  print(f\"Batch {idx + 1}: \", x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax891Il5MW45",
        "outputId": "f9db04d4-0d56-4211-c0c1-663299e9b900"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:  tensor([[ 2.3000, -1.1000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
            "Batch 2:  tensor([[-1.2000,  3.1000],\n",
            "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
            "Batch 3:  tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
          ]
        }
      ]
    }
  ]
}